# ------------------------------
# DB
# ------------------------------

VECTOR_STORE=pgvector-rs
PGVECTOR_HOST=pgvector
PGVECTOR_PORT=5432
PGVECTOR_USER=postgres
PGVECTOR_PASSWORD=feras123456
PGVECTOR_DATABASE=feras

# ------------------------------
# EMBEDDINGS
# ------------------------------

# huggingface or ollama
EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL_NAME=BAAI/bge-m3 # multilingual, no benchmark results available
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-large-instruct # best, requires instruction
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-large # better but slower
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-base # good, faster
# EMVEDDING_DIMENSION=768
EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-small # faster but less accurate
EMVEDDING_DIMENSION=384

# Increases retrieval latency
# RERANKING_MODEL_NAME=BAAI/bge-reranker-v2-m3

# required for huggingface provider
EMBEDDING_DEVICE=cuda

# ------------------------------
# LLM
# ------------------------------

# good: llama3.1:8b, glm4:9b, gemma2:9b-instruct, qwen2:7b-instruct
# bad: aya:8b, internlm2:7b-isntruct, falcon2:11b, mistral-v0.3:7b-instruct
LLM_PROVIDER=ollama
LLM_MODEL_NAME=llama3.1

# ------------------------------
# INGESTION
# controls the ingestion pipline, adjust based on machine
# ------------------------------

# depends on the embedding model's sequence length, and will affect the LLMs context window
CHUNK_CHARACTER_SIZE=1000
CHUNK_CHARACTER_OVERLAP=200
CHUNK_MIN_SIZE=100
CHUNK_QUEUE_MAX_SIZE=100000
CHUNK_INDEXING_BATCH_SIZE=50
# default to number of cores
# NUMBER_OF_CORES =
# default to number of cores
# BATCH_SIZE =

# ------------------------------
# Tracing
# ------------------------------

LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=