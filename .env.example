# ------------------------------
# DB
# ------------------------------

# pgvector-rs, neo4j
VECTOR_STORE=pgvector-rs

PGVECTOR_HOST=pgvector
PGVECTOR_PORT=5432
PGVECTOR_USER=postgres
PGVECTOR_PASSWORD=feras123456
PGVECTOR_DATABASE=feras

# supports hybrid search
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_URL=http://localhost:7687

# ------------------------------
# EMBEDDINGS
# ------------------------------

# huggingface, ollama, infinity
EMBEDDING_PROVIDER=infinity
EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-small # faster but less accurate
EMVEDDING_DIMENSION=384

# EMBEDDING_MODEL_NAME=BAAI/bge-m3 # multilingual, no benchmark results available
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-large-instruct # best, requires instruction
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-large # better but slower
# EMVEDDING_DIMENSION=1024
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-base # good, faster
# EMVEDDING_DIMENSION=768

# infinity, huggingface
RERANKING_PROVIDER=infinity
RERANKING_MODEL_NAME=BAAI/bge-reranker-v2-m3

# required for huggingface 
# cpu, cuda, mps 
EMBEDDING_DEVICE=cuda

# ------------------------------
# LLM
# ------------------------------

# good: llama3.1:8b, glm4:9b, gemma2:9b-instruct, qwen2:7b-instruct
# bad: aya:8b, internlm2:7b-isntruct, falcon2:11b, mistral-v0.3:7b-instruct
LLM_PROVIDER=ollama
LLM_MODEL_NAME=llama3.1

# LLM_PROVIDER=openai
# LLM_MODEL_NAME=gpt-4o
# OPENAI_API_KEY=xxx

# LLM_PROVIDER=openai
# LLM_MODEL_NAME=meta-llama/llama-3.1-8b-instruct:free
# OPENAI_API_KEY=xxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1

# LLM_PROVIDER=groq
# LLM_MODEL_NAME=llama-3.1-8b-instant
# GROQ_API_KEY=xxx

# LLM_PROVIDER=cohere
# LLM_MODEL_NAME=command-r-plus
# COHERE_API_KEY=xxx

# LLM_PROVIDER=together
# LLM_MODEL_NAME=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
# TOGETHER_API_KEY=xxx

# LLM_PROVIDER=fireworks
# LLM_MODEL_NAME=accounts/fireworks/models/llama-v3p1-8b-instruct
# FIREWORKS_API_KEY=xxx

# ------------------------------
# INGESTION
# controls the ingestion pipline, adjust based on machine
# ------------------------------

# depends on the embedding model's sequence length, and will affect the LLMs context window
CHUNK_CHARACTER_SIZE=1000
CHUNK_CHARACTER_OVERLAP=200
CHUNK_MIN_SIZE=100
CHUNK_QUEUE_MAX_SIZE=100000
CHUNK_INDEXING_BATCH_SIZE=50
# default to number of cores
# NUMBER_OF_CORES =
# default to number of cores
# BATCH_SIZE =

# ------------------------------
# Tracing
# ------------------------------

LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=